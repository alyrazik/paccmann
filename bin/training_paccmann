#!/usr/bin/env python3
# coding: utf-8
"""Script for `paccmann` training."""
import plac
import os
import json
import logging
import sys
import tensorflow as tf
import numpy as np
from datetime import datetime
from paccmann.models import paccmann_model_fn, MODEL_SPECIFICATION_FACTORY
from paccmann.datasets import (
    DATASETS_METADATA, validate_feature_names, CNV_FEATURES
)
from paccmann.learning import (
    train_input_fn, eval_input_fn, serving_input_with_params_fn
)

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logger = logging.getLogger('training_paccmann')


@plac.annotations(
    train_filepath=plac.Annotation(
        'Path to train data.',
        type=str
    ),
    eval_filepath=plac.Annotation(
        'Path to eval data.',
        type=str
    ),
    model_path=plac.Annotation(
        'Path where the model is stored.',
        type=str
    ),
    model_specification_fn_name=plac.Annotation(
        'Model specification function. Pick one of the following: {}.'.format(
            list(MODEL_SPECIFICATION_FACTORY.keys())
        ),
        type=str
    ),
    params_filepath=plac.Annotation(
        'Path to model params. Dictionary with parameters defining the model.',
        type=str
    ),
    feature_names=plac.Annotation(
        'Comma separated feature names. Select from the following: {}.'.format(
            list(DATASETS_METADATA.keys())
        ),
        type=str
    ),
    model_suffix=(
        'Suffix for the trained moedel.',
        'option', 'model_suffix', str
    ),
    save_checkpoints_steps=(
        'Steps before saving a checkpoint.',
        'option', 'save_checkpoints_steps', int
    ),
    eval_throttle_secs=(
        'Throttle seconds between evaluations.',
        'option', 'eval_throttle_secs', int
    ),
    train_steps=(
        'Number of training steps.',
        'option', 'train_steps', int
    ),
    batch_size=(
        'Batch size.',
        'option', 'batch_size', int
    ),
    learning_rate=(
        'Learning rate.',
        'option', 'learning_rate', float
    ),
    dropout=(
        'Dropout to be applied to set and dense layers.',
        'option', 'dropout', float
    ),
    buffer_size=(
        'Buffer size for data shuffling.',
        'option', 'buffer_size', int
    ),
    number_of_threads=(
        'Number of threads to be used in data processing.',
        'option', 'number_of_threads', int
    ),
    prefetch_buffer_size=(
        'Prefetch buffer size to allow pipelining.',
        'option', 'prefetch_buffer_size', int
    )
)
    
    #note that each model function have different set of allowable features to select from.
def main(
    train_filepath, eval_filepath,
    model_path, model_specification_fn_name,
    params_filepath, feature_names,
    save_checkpoints_steps=300,
    eval_throttle_secs=60,
    model_suffix='',
    train_steps=10000,
    batch_size=64,
    learning_rate=1e-3,
    dropout=0.5,
    buffer_size=20000,
    number_of_threads=1,
    prefetch_buffer_size=6
):
    """Run training of a `paccmann` model."""
    try:
        params = {} #an empty dictionary
        with open(params_filepath) as fp:
            params.update(json.load(fp)) #now we loaded the model parameters (from MCA.json for example) into the dictionary.
            

        feature_names = list(set(feature_names.split(','))) #we supply feature names, separated with commas in the function call
        # we take them into a list.
        #next part to attend to special thing regarding cnv. I am not sure what it is.
        if params.get('use_cnv_data', False):
            params.update({key: key for key in CNV_FEATURES})
            feature_names.extend(CNV_FEATURES)
        #this sorts the list we got for feature names.
        feature_names = sorted(feature_names)

        # Check if the model_specification_fn_name requested is available.
        #model specification function is like MCA, rnn for example. since they built a common architecture and used different
        #encoders 
        if model_specification_fn_name not in MODEL_SPECIFICATION_FACTORY: #paccmann.models had the value of this constant. 
            
            """
            MODEL_SPECIFICATION_FACTORY = {
                'dnn': dnn_fn,
                'rnn': rnn_fn,
                'scnn': scnn_fn,
                'sa': sa_fn,
                'ca': ca_fn,
                'mca': mca_fn
}
            """
            message = (
                '{} not present. Pick one of the following: {}'.format(
                    model_specification_fn_name,
                    list(MODEL_SPECIFICATION_FACTORY.keys())
                )
            )
            raise RuntimeError(message)
        # Check feature passed.
        # below are the list of available features.
        """
        # TFRecords metadata.
        DATASETS_METADATA = {
                'smiles_character_tokens': tf.FixedLenFeature((158), tf.int64),
                'smiles_atom_tokens': tf.FixedLenFeature((155), tf.int64),
                'fingerprints_256': tf.FixedLenFeature((256), tf.int64),
                'fingerprints_512': tf.FixedLenFeature((512), tf.int64),
                'targets_10': tf.FixedLenFeature(((10, 1)), tf.float32),
                'targets_20': tf.FixedLenFeature(((20, 1)), tf.float32),
                'targets_50': tf.FixedLenFeature(((50, 1)), tf.float32),
                'selected_genes_10': tf.FixedLenFeature((1121), tf.float32),
                'selected_genes_20': tf.FixedLenFeature((2128), tf.float32),
                'cnv_min': tf.FixedLenFeature((2128), tf.int64),
                'cnv_max': tf.FixedLenFeature((2128), tf.int64),
                'disrupt': tf.FixedLenFeature((2128), tf.int64),
                'zigosity': tf.FixedLenFeature((2128), tf.int64),
                'ic50': tf.FixedLenFeature((), tf.float32),
                'ic50_labels': tf.FixedLenFeature((), tf.int64),
            }

        """
        #below is used to validate the input feature names. 
        if len(feature_names) != len(validate_feature_names(feature_names)):
            message = (
                'feature_names={} provided not completely supported. Select '
                'from the following: {}.'.format(
                    feature_names,
                    list(DATASETS_METADATA.keys())
                )
            )
            raise RuntimeError(message)
        prefix = 'paccmann_model_{}_{}'.format(
            '-'.join(feature_names),
            model_specification_fn_name
        )
        
        #model_suffix is a not important details for now :) 9 Dec 2019 by AA.
        #it is just naming the model so if you'd save several , you'd uniquely name each
        #to be identifiable (AA Jan 2020)
        if len(model_suffix) < 1:
            model_suffix = '{:%Y-%m-%d_%H:%M:%S}'.format(
                datetime.now()
            )
        model_dir = os.path.join(
            model_path,
            '{}_{}'.format(prefix, model_suffix)
        )
        
        # Parameters.
        #let's load the model paramteres as specified by the main function inputs. 
        params.update({
            'feature_names': feature_names,
            'batch_size': batch_size,
            'train_filepath': train_filepath,
            'eval_filepath': eval_filepath,
            'learning_rate': learning_rate,
            'dropout': dropout,
            'buffer_size': buffer_size,
            'number_of_threads': (
                number_of_threads if number_of_threads > 1 else None
            ),
            'prefetch_buffer_size': prefetch_buffer_size
        })

        # Optionally update scaling parameters.
        #they didn't include this .json file and I don't know what it does. (AA) 
        ic50_normalization_params_filepath = os.path.join(
            train_filepath, 'ic50_normalization_params.json'
        )
        if os.path.exists(ic50_normalization_params_filepath):
            with open(ic50_normalization_params_filepath) as fp:
                params.update(json.load(fp))

        #view a message of all model parameters.
        logger.info('Model params:\n{}'.format(params))
        # Check if the feature requested are supported by the model.
        #collect all string paramters from the params dictionary. 
        string_parameter_values_set = set([
            value
            for value in params.values()
            if isinstance(value, str)
        ])
        #remeber feature_names are list of supplied features to the main function as input. 
        #intersection of supplied featue names with list of all model possible featuers 
        #(params is updated with a specific .json file of the specific model)
        #should yield all supplied feature names. 
        if (
            len(set(feature_names).intersection(
                string_parameter_values_set)
                ) != len(feature_names)
        ):
            message = (
                'feature_names provided not found in the params_filepath '
                'provided. '
                'Please make sure the feature_names are compatible with the '
                'model_specification_fn_name provided:\nparams_filepath={}\n'
                'feature_names={}\nmodel_specification_fn_name={}\n'
                'params={}'.format(
                    params_filepath, feature_names,
                    model_specification_fn_name, params.pop('feature_names')
                )
            )
            raise RuntimeError(message)
            
        #now the rest of the code is doing the actual work..All previous code was...
        #...attempting to make sure every thing is ready for this.    
        
        # Define exporter.
        #exporter exports the estimator 
        # it returns The string path to the exported directory
        #the class constructor is takes arguments as such:
        # __init__(
        #            name,
        #            serving_input_receiver_fn,
        #            assets_extra=None,
        #            as_text=False,
        #            exports_to_keep=5
        #        )
        """
        name: unique name of this Exporter that is going to be used in the export path.
        serving_input_receiver_fn: a function that takes no arguments and returns a ServingInputReceiver.
        assets_extra: An optional dict specifying how to populate the assets.extra directory within the exported SavedModel. Each key should give the destination path (including the filename) relative to the assets.extra directory. The corresponding value gives the full path of the source file to be copied. For example, the simple case of copying a single file without renaming it is specified as {'my_asset_file.txt': '/path/to/my_asset_file.txt'}.
        as_text: whether to write the SavedModel proto in text format. Defaults to False.
        exports_to_keep: Number of exports to keep. Older exports will be garbage-collected. Defaults to 5. Set to None to disable garbage collection.
        """
        
        
        
        exporter = tf.estimator.LatestExporter(
            '{}_exporter'.format(model_dir),
            lambda: serving_input_with_params_fn(params),
            exports_to_keep=None
        )
        # Define estimator.
        #takes model function, model directory and params
        estimator = tf.estimator.Estimator(
            model_fn=(
                lambda features, labels, mode, params: paccmann_model_fn(
                    features, labels, mode, params,
                    model_specification_fn=(
                        MODEL_SPECIFICATION_FACTORY[model_specification_fn_name]
                    )
                )
            ),
            model_dir=model_dir,
            params=params
        )
        if model_dir != None:
            # Check the training on `tensorboard`.
            logger.info(
                'Follow training evolution with: '
                'tensorboard --logdir {}'.format(
                    estimator.model_dir
                )
            )
            tf.estimator.RunConfig.save_checkpoints_steps = (
                save_checkpoints_steps
            )
            tf.estimator.RunConfig.save_checkpoints_secs = None
            tf.estimator.RunConfig.keep_checkpoint_max = 0

        # Define training and eval specifications.
        train_spec = tf.estimator.TrainSpec(
            input_fn=train_input_fn,
            max_steps=train_steps
        )
        eval_spec = tf.estimator.EvalSpec(
            input_fn=eval_input_fn,
            throttle_secs=eval_throttle_secs,
            steps=None,
            exporters=exporter
        )
        # Trainable parameters
        number_of_parameters = np.sum([
            np.prod(v.shape)
            for v in tf.trainable_variables()
        ])
        params.update({'trainable_parameters': number_of_parameters})

        # Dump params.
        os.makedirs(model_dir, exist_ok=True)
        with open(os.path.join(model_dir, 'model_params.json'), 'w') as fp:
            json.dump(params, fp)

        # Train and evaluate.
        tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
    except Exception:
        logger.exception('Exception occurred in running training_paccmann.py')


if __name__ == '__main__':
    plac.call(main)
